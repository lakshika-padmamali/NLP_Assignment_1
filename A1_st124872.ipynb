{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://nlpassignment1-horilwnh5asymqyy6bu9m6.streamlit.app/ link to web deployment app\n"
      ],
      "metadata": {
        "id": "0JJz6MO-oV-U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "II2PRO5fhN3e",
        "outputId": "03107425-b251-499d-923e-c9eaa8062ab8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import nltk\n",
        "import matplotlib\n",
        "nltk.download('brown')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.__version__, torch.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkDXP5FYhXIY",
        "outputId": "7de4b7a1-c66d-4ad8-f24e-cca6957fef7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('1.26.4', '2.5.1+cu121')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import brown\n",
        "\n",
        "# Corpus containing documents from the 'earn' category\n",
        "corpus = brown.sents()\n",
        "\n",
        "# Limit the corpus to the first 1000 sentences\n",
        "corpus = [[word.lower() for word in sentence] for sentence in corpus]\n",
        "corpus = corpus[:1000]"
      ],
      "metadata": {
        "id": "r4Ub242CheRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get word sequences and unique words\n",
        "flatten = lambda l: [item for sublist in l for item in sublist]\n",
        "vocab = list(set(flatten(corpus)))"
      ],
      "metadata": {
        "id": "WY-foun3hng1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Numericalization\n",
        "word2index = {w: i for i, w in enumerate(vocab)}\n",
        "# Vocab size\n",
        "voc_size = len(vocab)\n",
        "print(voc_size)\n",
        "\n",
        "# Append UNK\n",
        "vocab.append('<UNK>')\n",
        "word2index['<UNK>'] = 0\n",
        "\n",
        "index2word = {v:k for k, v in word2index.items()}"
      ],
      "metadata": {
        "id": "x9qT0KD-hvwa",
        "outputId": "ba16e662-9a27-4e25-a360-2a5f9a235dbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4272\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from itertools import combinations_with_replacement\n",
        "\n",
        "# Index the corpus\n",
        "X_i = Counter(flatten(corpus))\n",
        "\n",
        "# Function to generate skip-grams dynamically\n",
        "def generate_skipgrams_dynamic(corpus, window_size=2):\n",
        "    skip_grams = []\n",
        "    for doc in corpus:\n",
        "        for i in range(window_size, len(doc) - window_size):\n",
        "            center = doc[i]\n",
        "            outside = []\n",
        "            for j in range(1, window_size + 1):  # Dynamic range based on window size\n",
        "                if i - j >= 0:\n",
        "                    outside.append(doc[i - j])  # Words to the left\n",
        "                if i + j < len(doc):\n",
        "                    outside.append(doc[i + j])  # Words to the right\n",
        "            for each_out in outside:\n",
        "                skip_grams.append((center, each_out))\n",
        "    return skip_grams\n",
        "\n",
        "# Generate skip-grams with dynamic window size\n",
        "window_size = 2  # Default window size\n",
        "skip_grams = generate_skipgrams_dynamic(corpus, window_size)\n",
        "X_ik_skipgrams = Counter(skip_grams)\n",
        "\n",
        "# Weighting function\n",
        "def weighting(w_i, w_j, X_ik):\n",
        "    try:\n",
        "        x_ij = X_ik[(w_i, w_j)]\n",
        "    except:\n",
        "        x_ij = 1\n",
        "    x_max = 100\n",
        "    alpha = 0.75\n",
        "    if x_ij < x_max:\n",
        "        result = (x_ij / x_max)**alpha\n",
        "    else:\n",
        "        result = 1\n",
        "    return result\n",
        "\n",
        "# Generate co-occurrence matrix and weighting dictionary\n",
        "X_ik = {}  # For co-occurrences\n",
        "weighting_dic = {}  # For scaled weights\n",
        "\n",
        "for bigram in combinations_with_replacement(vocab, 2):\n",
        "    if X_ik_skipgrams.get(bigram) is not None:  # Matches\n",
        "        co_occer = X_ik_skipgrams[bigram]  # Count from skip-grams\n",
        "        X_ik[bigram] = co_occer + 1  # Add 1 for stability\n",
        "        X_ik[(bigram[1], bigram[0])] = co_occer + 1  # Symmetry\n",
        "    weighting_dic[bigram] = weighting(bigram[0], bigram[1], X_ik)\n",
        "    weighting_dic[(bigram[1], bigram[0])] = weighting(bigram[1], bigram[0], X_ik)\n"
      ],
      "metadata": {
        "id": "rHm39VAA0mqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create pairs of center word, and outside word\n",
        "\n",
        "def random_batch(batch_size, corpus, window_size=2):\n",
        "    skipgrams = []\n",
        "    for doc in corpus:\n",
        "        for i in range(window_size, len(doc)-window_size):\n",
        "            center = word2index[doc[i]]\n",
        "            outside = [word2index[doc[i-j]] for j in range(-window_size, window_size+1) if j != 0]\n",
        "            for each_out in outside:\n",
        "                skipgrams.append([center, each_out])\n",
        "\n",
        "    random_index = np.random.choice(range(len(skipgrams)), batch_size, replace=False)\n",
        "    inputs, labels = [], []\n",
        "    for index in random_index:\n",
        "        inputs.append([skipgrams[index][0]])\n",
        "        labels.append([skipgrams[index][1]])\n",
        "\n",
        "    return np.array(inputs), np.array(labels)\n",
        "\n",
        "\n",
        "x, y = random_batch(2, corpus)\n",
        "import math\n",
        "\n",
        "def random_batch_glove(batch_size, word_sequence, skip_grams, X_ik, weighting_dic):\n",
        "\n",
        "    #convert to id since our skip_grams is word, not yet id\n",
        "    skip_grams_id = [(word2index[skip_gram[0]], word2index[skip_gram[1]]) for skip_gram in skip_grams]\n",
        "\n",
        "    random_inputs = []\n",
        "    random_labels = []\n",
        "    random_coocs  = []\n",
        "    random_weightings = []\n",
        "    random_index = np.random.choice(range(len(skip_grams_id)), batch_size, replace=False) #randomly pick without replacement\n",
        "\n",
        "    for i in random_index:\n",
        "        random_inputs.append([skip_grams_id[i][0]])  # target, e.g., 2\n",
        "        random_labels.append([skip_grams_id[i][1]])  # context word, e.g., 3\n",
        "\n",
        "        #get cooc\n",
        "        pair = skip_grams[i]\n",
        "        try:\n",
        "            cooc = X_ik[pair]\n",
        "        except:\n",
        "            cooc = 1\n",
        "        random_coocs.append([math.log(cooc)])\n",
        "\n",
        "        #get weighting\n",
        "        weighting = weighting_dic[pair]\n",
        "        random_weightings.append([weighting])\n",
        "\n",
        "    return np.array(random_inputs), np.array(random_labels), np.array(random_coocs), np.array(random_weightings)\n",
        "x.shape  #batch_size, 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDPYazbw1sCa",
        "outputId": "b70b01c7-29c3-45b4-82bb-33133ef49061"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOCxhWIb1zQs",
        "outputId": "fc779899-5924-4564-8d17-e1eac8a7dc8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hY8ezAN910wW",
        "outputId": "2a02ed52-d0ac-4e11-f075-94afeb5b60ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4273"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = nn.Embedding(63314, 2)\n",
        "x_tensor = torch.LongTensor(x)\n",
        "embedding(x_tensor).shape  #(batch_size, 1, emb_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1A64X5aT15-O",
        "outputId": "abedc65b-bf6e-4b91-b39a-2b04271fa6e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 1, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Skipgram(nn.Module):\n",
        "\n",
        "    def __init__(self, voc_size, emb_size):\n",
        "        super(Skipgram, self).__init__()\n",
        "        self.embedding_center  = nn.Embedding(voc_size, emb_size)\n",
        "        self.embedding_outside = nn.Embedding(voc_size, emb_size)\n",
        "\n",
        "    def forward(self, center, outside, all_vocabs):\n",
        "        center_embedding     = self.embedding_center(center)  #(batch_size, 1, emb_size)\n",
        "        outside_embedding    = self.embedding_center(outside) #(batch_size, 1, emb_size)\n",
        "        all_vocabs_embedding = self.embedding_center(all_vocabs) #(batch_size, voc_size, emb_size)\n",
        "\n",
        "        top_term = torch.exp(outside_embedding.bmm(center_embedding.transpose(1, 2)).squeeze(2))\n",
        "        #batch_size, 1, emb_size) @ (batch_size, emb_size, 1) = (batch_size, 1, 1) = (batch_size, 1)\n",
        "\n",
        "        lower_term = all_vocabs_embedding.bmm(center_embedding.transpose(1, 2)).squeeze(2)\n",
        "        #batch_size, voc_size, emb_size) @ (batch_size, emb_size, 1) = (batch_size, voc_size, 1) = (batch_size, voc_size)\n",
        "\n",
        "        lower_term_sum = torch.sum(torch.exp(lower_term), 1)  #(batch_size, 1)\n",
        "\n",
        "        loss = -torch.mean(torch.log(top_term / lower_term_sum))  #scalar\n",
        "\n",
        "        return loss\n"
      ],
      "metadata": {
        "id": "dpDPPgrP19JJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SkipgramNegSampling(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, emb_size):\n",
        "        super(SkipgramNegSampling, self).__init__()\n",
        "        self.embedding_center = nn.Embedding(vocab_size, emb_size) # center embedding\n",
        "        self.embedding_outside = nn.Embedding(vocab_size, emb_size) # out embedding\n",
        "        self.logsigmoid = nn.LogSigmoid()\n",
        "\n",
        "    def forward(self, center_words, target_words, negative_words):\n",
        "        center_embeds = self.embedding_center(center_words) # [batch_size, 1, emb_size]\n",
        "        target_embeds = self.embedding_outside(target_words) # [batch_size, 1, emb_size]\n",
        "        neg_embeds    = -self.embedding_outside(negative_words) # [batch_size, num_neg, emb_size]\n",
        "\n",
        "        positive_score = target_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
        "        #[batch_size, 1, emb_size] @ [batch_size, emb_size, 1] = [batch_size, 1, 1] = [batch_size, 1]\n",
        "\n",
        "        negative_score = neg_embeds.bmm(center_embeds.transpose(1, 2))\n",
        "        #[batch_size, k, emb_size] @ [batch_size, emb_size, 1] = [batch_size, k, 1]\n",
        "\n",
        "        loss = self.logsigmoid(positive_score) + torch.sum(self.logsigmoid(negative_score), 1)\n",
        "\n",
        "        return -torch.mean(loss)\n",
        "\n",
        "    def prediction(self, inputs):\n",
        "        embeds = self.embedding_v(inputs)\n",
        "\n",
        "        return embeds"
      ],
      "metadata": {
        "id": "rYqEQi921_sK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Glove(nn.Module):\n",
        "\n",
        "    def __init__(self, voc_size, emb_size):\n",
        "        super(Glove, self).__init__()\n",
        "        self.embedding_center  = nn.Embedding(voc_size, emb_size)\n",
        "        self.embedding_outside = nn.Embedding(voc_size, emb_size)\n",
        "\n",
        "        self.center_bias       = nn.Embedding(voc_size, 1)\n",
        "        self.outside_bias      = nn.Embedding(voc_size, 1)\n",
        "\n",
        "    def forward(self, center, outside, coocs, weighting):\n",
        "        center_embeds  = self.embedding_center(center) #(batch_size, 1, emb_size)\n",
        "        outside_embeds = self.embedding_outside(outside) #(batch_size, 1, emb_size)\n",
        "\n",
        "        center_bias    = self.center_bias(center).squeeze(1)\n",
        "        target_bias    = self.outside_bias(outside).squeeze(1)\n",
        "\n",
        "        inner_product  = outside_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
        "        #(batch_size, 1, emb_size) @ (batch_size, emb_size, 1) = (batch_size, 1, 1) = (batch_size, 1)\n",
        "\n",
        "        loss = weighting * torch.pow(inner_product + center_bias + target_bias - coocs, 2)\n",
        "\n",
        "        return torch.sum(loss)"
      ],
      "metadata": {
        "id": "pBGs79U02EjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yejaSUHoKFid",
        "outputId": "e3f6fc40-5bea-4852-fa39-1f5010f6307b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-18 01:12:49--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2025-01-18 01:12:49--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2025-01-18 01:12:49--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  4.96MB/s    in 2m 41s  \n",
            "\n",
            "2025-01-18 01:15:30 (5.12 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.test.utils import datapath\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "\n",
        "# setting the dataset\n",
        "glove_file = datapath('/content/glove.6B.100d.txt')\n",
        "model_gensim = KeyedVectors.load_word2vec_format(glove_file, binary=False, no_header=True)"
      ],
      "metadata": {
        "id": "UhYoDIGs2IQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 2\n",
        "voc_size   = len(vocab)\n",
        "emb_size = 2\n",
        "\n",
        "def prepare_sequence(seq, word2index):\n",
        "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
        "    return torch.LongTensor(idxs)\n",
        "\n",
        "all_vocabs = prepare_sequence(list(vocab), word2index).expand(batch_size, voc_size)\n",
        "all_vocabs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kFHfNdb8WIJ",
        "outputId": "3edcae14-511c-4550-c869-ad966903e24d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[   0,    1,    2,  ..., 4270, 4271,    0],\n",
              "        [   0,    1,    2,  ..., 4270, 4271,    0]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_skipgram = Skipgram(voc_size, emb_size)\n",
        "model_skipgram"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDToiJRt8ZOh",
        "outputId": "2a024f34-9ce4-4ed7-be58-c613f7350db5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Skipgram(\n",
              "  (embedding_center): Embedding(4273, 2)\n",
              "  (embedding_outside): Embedding(4273, 2)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_skipgram_neg = Skipgram(voc_size, emb_size)\n",
        "model_skipgram_neg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H92vloiB8cfE",
        "outputId": "8f8a7c22-0da4-43d0-84ad-2beb15473a0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Skipgram(\n",
              "  (embedding_center): Embedding(4273, 2)\n",
              "  (embedding_outside): Embedding(4273, 2)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_glove = Glove(voc_size, emb_size)\n",
        "model_glove"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnfNXcoH8cIe",
        "outputId": "654ea180-7d20-4775-a2f9-d96331dc6706"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Glove(\n",
              "  (embedding_center): Embedding(4273, 2)\n",
              "  (embedding_outside): Embedding(4273, 2)\n",
              "  (center_bias): Embedding(4273, 1)\n",
              "  (outside_bias): Embedding(4273, 1)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_tensor = torch.LongTensor(x)\n",
        "label_tensor = torch.LongTensor(y)\n",
        "loss_skipgram = model_skipgram(input_tensor, label_tensor, all_vocabs)\n",
        "loss_skipgram_neg = model_skipgram_neg(input_tensor, label_tensor, all_vocabs)\n",
        "# x, y, cooc, weighting = random_batch_glove(batch_size, corpus, skip_grams, X_ik, weighting_dic)\n",
        "\n",
        "# loss_glove = model_glove(torch.LongTensor(x), torch.LongTensor(y), torch.LongTensor(cooc), torch.LongTensor(weighting))\n",
        "batch_size = 2\n",
        "emb_size   = 2\n",
        "model_skipgram     = Skipgram(voc_size, emb_size)\n",
        "optimizer_skipgram  = optim.Adam(model_skipgram.parameters(), lr=0.001)\n",
        "optimizer_skipgram_neg  = optim.Adam(model_skipgram_neg.parameters(), lr=0.001)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_glove = optim.Adam(model_glove.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "amx3EvJC8etG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n",
        "num_epochs = 10\n",
        "total_start = time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    start = time.time()\n",
        "\n",
        "    #get batch\n",
        "    input_batch, label_batch = random_batch(batch_size, corpus)\n",
        "\n",
        "    input_tensor = torch.LongTensor(input_batch)\n",
        "    label_tensor = torch.LongTensor(label_batch)\n",
        "\n",
        "    #predict\n",
        "    loss_skipgram= model_skipgram(input_tensor, label_tensor, all_vocabs)\n",
        "\n",
        "    #backprogate\n",
        "    optimizer_skipgram.zero_grad()\n",
        "    loss_skipgram.backward()\n",
        "\n",
        "    #update alpha\n",
        "    optimizer_skipgram.step()\n",
        "    end = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start, end)\n",
        "\n",
        "    #print the loss_skipgram_positive\n",
        "    # if (epoch + 1) % 1000 == 0:\n",
        "    print(\"Positive Skigram\")\n",
        "    print(f\"Epoch {epoch+1:6.0f} | Loss: {loss_skipgram:2.6f}| time: {epoch_mins}m {epoch_secs}s\")\n",
        "# Record the ending time\n",
        "total_end = time.time()\n",
        "\n",
        "# Calculate and print the total runtime\n",
        "total_runtime = total_end - total_start\n",
        "print(f\"Total runtime: {total_runtime:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWpaluT48hb1",
        "outputId": "e678a4ab-3774-4acc-c198-a77c9ba7ede1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive Skigram\n",
            "Epoch      1 | Loss: 8.643091| time: 0m 0s\n",
            "Positive Skigram\n",
            "Epoch      2 | Loss: 9.371527| time: 0m 0s\n",
            "Positive Skigram\n",
            "Epoch      3 | Loss: 12.445749| time: 0m 0s\n",
            "Positive Skigram\n",
            "Epoch      4 | Loss: 8.952690| time: 0m 0s\n",
            "Positive Skigram\n",
            "Epoch      5 | Loss: 9.025588| time: 0m 0s\n",
            "Positive Skigram\n",
            "Epoch      6 | Loss: 9.195283| time: 0m 0s\n",
            "Positive Skigram\n",
            "Epoch      7 | Loss: 7.735710| time: 0m 0s\n",
            "Positive Skigram\n",
            "Epoch      8 | Loss: 10.200654| time: 0m 0s\n",
            "Positive Skigram\n",
            "Epoch      9 | Loss: 9.624472| time: 0m 0s\n",
            "Positive Skigram\n",
            "Epoch     10 | Loss: 11.756741| time: 0m 0s\n",
            "Total runtime: 1.99 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "\n",
        "\n",
        "total_start = time.time()\n",
        "for epoch in range(num_epochs):\n",
        "    start = time.time()\n",
        "\n",
        "    #get batch\n",
        "    input_batch, label_batch = random_batch(batch_size, corpus)\n",
        "\n",
        "    input_tensor = torch.LongTensor(input_batch)\n",
        "    label_tensor = torch.LongTensor(label_batch)\n",
        "\n",
        "    #predict\n",
        "    loss_skipgram_negative = model_skipgram_neg(input_tensor, label_tensor, all_vocabs)\n",
        "\n",
        "    #backprogate\n",
        "    optimizer_skipgram_neg.zero_grad()\n",
        "    loss_skipgram_negative.backward()\n",
        "\n",
        "    #update alpha\n",
        "    optimizer_skipgram_neg.step()\n",
        "\n",
        "    end = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start, end)\n",
        "\n",
        "    #print the loss_skipgram_positive\n",
        "    # if (epoch + 1) % 1000 == 0:\n",
        "    print(\"Negative Skigram\")\n",
        "    print(f\"Epoch {epoch+1:6.0f} | Loss: {loss_skipgram_negative:2.6f} | time: {epoch_mins}m {epoch_secs}s\")\n",
        "# Record the ending time\n",
        "total_end = time.time()\n",
        "\n",
        "# Calculate and print the total runtime\n",
        "total_runtime = total_end - total_start\n",
        "print(f\"Total runtime: {total_runtime:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ks9ueFWU8lNB",
        "outputId": "24199bac-dae6-4124-8d29-e73a4e44c2ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Negative Skigram\n",
            "Epoch      1 | Loss: 10.175230 | time: 0m 0s\n",
            "Negative Skigram\n",
            "Epoch      2 | Loss: 8.720857 | time: 0m 0s\n",
            "Negative Skigram\n",
            "Epoch      3 | Loss: 9.671392 | time: 0m 0s\n",
            "Negative Skigram\n",
            "Epoch      4 | Loss: 9.040108 | time: 0m 0s\n",
            "Negative Skigram\n",
            "Epoch      5 | Loss: 9.543203 | time: 0m 0s\n",
            "Negative Skigram\n",
            "Epoch      6 | Loss: 10.529750 | time: 0m 0s\n",
            "Negative Skigram\n",
            "Epoch      7 | Loss: 7.815857 | time: 0m 0s\n",
            "Negative Skigram\n",
            "Epoch      8 | Loss: 9.118710 | time: 0m 0s\n",
            "Negative Skigram\n",
            "Epoch      9 | Loss: 8.788888 | time: 0m 0s\n",
            "Negative Skigram\n",
            "Epoch     10 | Loss: 8.751039 | time: 0m 0s\n",
            "Total runtime: 1.89 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    start = time.time()\n",
        "\n",
        "    #get batch\n",
        "    input_batch, label_batch = random_batch(batch_size, corpus)\n",
        "\n",
        "    input_batch, target_batch, cooc_batch, weighting_batch = random_batch_glove(batch_size, corpus, skip_grams, X_ik, weighting_dic)\n",
        "    input_batch  = torch.LongTensor(input_batch)         #[batch_size, 1]\n",
        "    target_batch = torch.LongTensor(target_batch)        #[batch_size, 1]\n",
        "    cooc_batch   = torch.FloatTensor(cooc_batch)         #[batch_size, 1]\n",
        "    weighting_batch = torch.FloatTensor(weighting_batch)\n",
        "\n",
        "    #predict\n",
        "    loss_glove = model_glove(input_batch, target_batch, cooc_batch, weighting_batch)\n",
        "\n",
        "    #backprogate\n",
        "    optimizer_glove.zero_grad()\n",
        "    loss_glove.backward()\n",
        "\n",
        "    #update alpha\n",
        "    optimizer_glove.step()\n",
        "\n",
        "    end = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start, end)\n",
        "\n",
        "    #print the loss_skipgram_positive\n",
        "    # if (epoch + 1) % 1000 == 0:\n",
        "    print(\"Glove\")\n",
        "    print(f\"Epoch {epoch+1:6.0f} | Loss: {loss_glove:2.6f} | time: {epoch_mins}m {epoch_secs}s\")\n",
        "total_runtime = total_end - total_start\n",
        "print(f\"Total runtime: {total_runtime:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Uo9WEwq8r2A",
        "outputId": "dce41b71-f790-43bc-a42f-86d5a3df0efe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Glove\n",
            "Epoch      1 | Loss: 1.239861 | time: 0m 0s\n",
            "Glove\n",
            "Epoch      2 | Loss: 1.200808 | time: 0m 0s\n",
            "Glove\n",
            "Epoch      3 | Loss: 1.010004 | time: 0m 0s\n",
            "Glove\n",
            "Epoch      4 | Loss: 21.652637 | time: 0m 0s\n",
            "Glove\n",
            "Epoch      5 | Loss: 0.322434 | time: 0m 0s\n",
            "Glove\n",
            "Epoch      6 | Loss: 0.217125 | time: 0m 0s\n",
            "Glove\n",
            "Epoch      7 | Loss: 1.000654 | time: 0m 0s\n",
            "Glove\n",
            "Epoch      8 | Loss: 0.203099 | time: 0m 0s\n",
            "Glove\n",
            "Epoch      9 | Loss: 0.078265 | time: 0m 0s\n",
            "Glove\n",
            "Epoch     10 | Loss: 2.034088 | time: 0m 0s\n",
            "Total runtime: 1.89 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embed(model, word):\n",
        "    try:\n",
        "        # Find the index\n",
        "        index = word2index[word]\n",
        "    except:\n",
        "        # if not found give the index of unknown token\n",
        "        index = word2index['<UNK>']\n",
        "\n",
        "    # get the word in terms of tensor\n",
        "    word = torch.LongTensor([word2index[word]])\n",
        "     # embed the center and the outside word and then find the final embed\n",
        "    embed_c = model.embedding_center(word)\n",
        "    embed_o = model.embedding_outside(word)\n",
        "    embed   = (embed_c + embed_o) / 2\n",
        "\n",
        "\n",
        "    return embed[0][0].item(), embed[0][1].item()\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def get_embed_for_corpus(model, words):\n",
        "    embeddings = {}\n",
        "\n",
        "    for word in words:\n",
        "        try:\n",
        "            index = word2index[word]\n",
        "        except KeyError:\n",
        "            index = word2index['<UNK>']\n",
        "\n",
        "        word_tensor = torch.LongTensor([index])\n",
        "\n",
        "        embed_c = model.embedding_center(word_tensor)\n",
        "        embed_o = model.embedding_outside(word_tensor)\n",
        "        embed = (embed_c + embed_o) / 2\n",
        "\n",
        "        # return as dictionary with key as the word and value as the array of its embedding\n",
        "        embeddings[word] = np.array([embed[0][0].item(), embed[0][1].item()])\n",
        "\n",
        "    return embeddings"
      ],
      "metadata": {
        "id": "wK3GgdKr8wY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute cosine similarity between two vectors\n",
        "def cosine_similarity(A, B):\n",
        "    # Calculate dot product of the two vectors\n",
        "    dot_product = np.dot(A, B)\n",
        "    # Compute the norm (magnitude) of each vector\n",
        "    norm_a = np.linalg.norm(A)\n",
        "    norm_b = np.linalg.norm(B)\n",
        "    # Calculate the cosine similarity\n",
        "    similarity = dot_product / (norm_a * norm_b)\n",
        "    return similarity\n",
        "\n",
        "# Function to compute cosine similarity for all words in the corpus relative to a target word\n",
        "def cosine_similarity_for_corpus(embeddings, target_word):\n",
        "    # List to store pairs of (word, cosine_similarity) for each word in the corpus\n",
        "    similarities = []\n",
        "\n",
        "    # Get the index of the target word; fallback to '<UNK>' if the word is not found\n",
        "    target_index = word2index.get(target_word, word2index['<UNK>'])\n",
        "\n",
        "    # Retrieve the vector representation of the target word\n",
        "    target_vector = embeddings[target_index]\n",
        "\n",
        "    # Iterate through all words and their corresponding vectors in the embeddings\n",
        "    for word, vector in embeddings.items():\n",
        "        # Calculate the cosine similarity between the target vector and the current word vector\n",
        "        similarity = cosine_similarity(target_vector, vector)\n",
        "\n",
        "        # Store the word and its similarity score as a tuple in the list\n",
        "        similarities.append((word, similarity))\n",
        "\n",
        "    return similarities\n"
      ],
      "metadata": {
        "id": "vCRo1SkG84En"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "7U71yDvdLW6C",
        "outputId": "5754d323-997b-4ad0-deea-7063c2d55be8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6e0ab983-59ea-4043-b83a-71e5a506f325\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6e0ab983-59ea-4043-b83a-71e5a506f325\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving word-test.v1.txt to word-test.v1.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the path to your .txt file\n",
        "file_path = '/content/word-test.v1.txt'\n",
        "\n",
        "# Read the content of the file\n",
        "with open(file_path, 'r') as file:\n",
        "    # Skip the first line\n",
        "    file.readline()\n",
        "\n",
        "    # Read the remaining content of the file\n",
        "    file_content = file.readlines()\n",
        "\n",
        "# Initialize variables to store relevant lines\n",
        "total_corpus = []\n",
        "\n",
        "# Variable to keep track of the current heading\n",
        "current_heading = None\n",
        "\n",
        "# Iterate through each line in the file content\n",
        "for line in file_content:\n",
        "    # Check if the line is a heading\n",
        "    if line.startswith(':'):\n",
        "        current_heading = line.strip()\n",
        "    else:\n",
        "        # Split the line into individual words and convert to lowercase\n",
        "        words = [word.lower() for word in line.strip().split()]\n",
        "        total_corpus.append(words)\n",
        "\n",
        "# Initialize variables to store relevant lines\n",
        "capital_common_countries = []\n",
        "past_tense = []\n",
        "\n",
        "# Variable to keep track of the current heading\n",
        "current_heading = None\n",
        "\n",
        "# Iterate through each line in the file content\n",
        "for line in file_content:\n",
        "    # Check if the line is a heading\n",
        "    if line.startswith(':'):\n",
        "        current_heading = line.strip()\n",
        "    elif current_heading == ': capital-common-countries':\n",
        "        # Split the line into individual words and convert to lowercase\n",
        "        words = [word.lower() for word in line.strip().split()]\n",
        "        capital_common_countries.append(words)\n",
        "    elif current_heading == ': gram7-past-tense':\n",
        "        # Split the line into individual words and convert to lowercase\n",
        "        words = [word.lower() for word in line.strip().split()]\n",
        "        past_tense.append(words)"
      ],
      "metadata": {
        "id": "XebkXpkc87L3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten the 2D list into a list of lists\n",
        "flattened_list_of_country = [word for pair in capital_common_countries for word in pair]\n",
        "\n",
        "# Wrap the flattened list in another list\n",
        "resulting_capital_list = [flattened_list_of_country]\n",
        "\n",
        "# Flatten the 2D list into a list of lists\n",
        "flattened_list_of_past_tense = [word for pair in past_tense for word in pair]\n",
        "\n",
        "# Wrap the flattened list in another list\n",
        "resulting_capital_list = [flattened_list_of_country]\n",
        "resulting_past_tense_list = [flattened_list_of_past_tense]\n",
        "\n",
        "# Flatten the 2D list into a list of lists\n",
        "flattened_list_total_words = [word for pair in total_corpus for word in pair]\n",
        "# Wrap the flattened list in another list\n",
        "resulting_total_corpus = [flattened_list_total_words]\n",
        "\n",
        "\n",
        "flatten = lambda l: [item for sublist in l for item in sublist]\n",
        "capital_list = list(set(flatten(resulting_capital_list)))\n",
        "past_tense_list = list(set(flatten(resulting_past_tense_list)))\n",
        "whole_corpus = list(set(flatten(resulting_total_corpus)))"
      ],
      "metadata": {
        "id": "dfNFVQjR9a1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store the embeddings\n",
        "embed_capital_glove = get_embed_for_corpus(model_glove, capital_list)\n",
        "embed_capital_skipgram = get_embed_for_corpus(model_skipgram, capital_list)\n",
        "embed_capital_skipgram_neg = get_embed_for_corpus(model_skipgram_neg, capital_list)\n",
        "\n",
        "embed_past_tense_glove = get_embed_for_corpus(model_glove, past_tense_list)\n",
        "embed_past_tense_skipgram = get_embed_for_corpus(model_skipgram, past_tense_list)\n",
        "embed_past_tense_skipgram_neg = get_embed_for_corpus(model_skipgram_neg, past_tense_list)\n",
        "\n",
        "embed_total_glove = get_embed_for_corpus(model_glove, whole_corpus)\n",
        "embed_whole_skipgram = get_embed_for_corpus(model_skipgram, whole_corpus)\n",
        "embed_whole_skipgram_neg = get_embed_for_corpus(model_skipgram_neg, whole_corpus)\n",
        "# y_pred for glove for the capital list\n",
        "y_pred_glove_country = []\n",
        "\n",
        "for i in capital_common_countries:\n",
        "    y = embed_capital_glove[i[1]] - embed_capital_glove[i[0]] + embed_capital_glove[i[2]]\n",
        "    y_pred_glove_country.append(y)\n",
        "# y_pred for glove for the past tense list\n",
        "y_pred_glove_past = []\n",
        "\n",
        "for i in past_tense:\n",
        "    y = embed_past_tense_glove[i[1]] - embed_past_tense_glove[i[0]] + embed_past_tense_glove[i[2]]\n",
        "    y_pred_glove_past.append(y)\n",
        "# y_pred for skipgram negative sampling for the capital list\n",
        "y_pred_neg_samp_country = []\n",
        "\n",
        "for i in capital_common_countries:\n",
        "    y = embed_capital_skipgram_neg[i[1]] - embed_capital_skipgram_neg[i[0]] + embed_capital_skipgram_neg[i[2]]\n",
        "    y_pred_neg_samp_country.append(y)"
      ],
      "metadata": {
        "id": "OP6mTAAR9dxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# y_pred for skip-gram with negative sampling for the past tense list\n",
        "y_pred_neg_samp_past = []\n",
        "\n",
        "for i in past_tense:\n",
        "    # Calculate predicted vector for past tense using negative sampling embeddings\n",
        "    y = embed_past_tense_skipgram_neg[i[0]] - embed_past_tense_skipgram_neg[i[0]] + embed_past_tense_skipgram_neg[i[2]]\n",
        "    y_pred_neg_samp_past.append(y)\n",
        "\n",
        "# y_pred for skip-gram with positive sampling for the country list\n",
        "y_pred_positive_samp_country = []\n",
        "\n",
        "for i in capital_common_countries:\n",
        "    # Calculate predicted vector for capital-common-countries using positive sampling embeddings\n",
        "    y = embed_capital_skipgram[i[1]] - embed_capital_skipgram[i[0]] + embed_capital_skipgram[i[2]]\n",
        "    y_pred_positive_samp_country.append(y)\n",
        "\n",
        "# y_pred for skip-gram with positive sampling for the past tense list\n",
        "y_pred_positive_past_tense = []\n",
        "\n",
        "for i in past_tense:\n",
        "    # Calculate predicted vector for past tense using positive sampling embeddings\n",
        "    y = embed_past_tense_skipgram[i[1]] - embed_past_tense_skipgram[i[0]] + embed_past_tense_skipgram[i[2]]\n",
        "    y_pred_positive_past_tense.append(y)\n",
        "\n",
        "# Function to compute cosine similarity between two vectors\n",
        "def cosine_similarity(A, B):\n",
        "    # Calculate dot product of the two vectors\n",
        "    dot_product = np.dot(A, B)\n",
        "    # Compute the norm (magnitude) of each vector\n",
        "    norm_a = np.linalg.norm(A)\n",
        "    norm_b = np.linalg.norm(B)\n",
        "    # Calculate the cosine similarity\n",
        "    similarity = dot_product / (norm_a * norm_b)\n",
        "    return similarity\n",
        "\n",
        "# Function to find the word with the maximum cosine similarity for each vector in y_pred\n",
        "def find_max_cosine_words(y_pred, embeddings):\n",
        "    \"\"\"\n",
        "    Find the word with the maximum cosine similarity for each vector in y_pred.\n",
        "\n",
        "    Parameters:\n",
        "    - y_pred: List of vectors for which to find the max cosine similarity words.\n",
        "    - embeddings: Dictionary of word embeddings.\n",
        "\n",
        "    Returns:\n",
        "    - List of words with the maximum cosine similarity for each vector in y_pred.\n",
        "    \"\"\"\n",
        "    max_cosine_words = []\n",
        "\n",
        "    for j in range(len(y_pred)):\n",
        "        max_cosine = -1\n",
        "        max_cosine_word = \"\"\n",
        "\n",
        "        # Iterate through all words in the embeddings dictionary\n",
        "        for i in embeddings.keys():\n",
        "            # Calculate cosine similarity between the predicted vector and the current word vector\n",
        "            cosine_temp = cosine_similarity(y_pred[j], embeddings[i])\n",
        "\n",
        "            # Update the word with the highest cosine similarity\n",
        "            if cosine_temp > max_cosine:\n",
        "                max_cosine_word = i\n",
        "                max_cosine = cosine_temp\n",
        "\n",
        "        max_cosine_words.append(max_cosine_word)\n",
        "\n",
        "    return max_cosine_words\n",
        "\n",
        "# Usage: Compute syntactic predictions for models\n",
        "cosine_neg_samp_syntatical = find_max_cosine_words(y_pred_neg_samp_country, embed_capital_skipgram_neg)\n",
        "cosine_positive_samp_syntatical = find_max_cosine_words(y_pred_positive_samp_country, embed_capital_skipgram)\n",
        "cosine_glove_syntatical = find_max_cosine_words(y_pred_glove_country, embed_capital_glove)\n",
        "\n",
        "# Function to find the next top-N similar words for a specific word using cosine similarity\n",
        "from heapq import nlargest\n",
        "\n",
        "def find_next_10_cosine_words_for_word(target_word, embeddings, top_n=10):\n",
        "    \"\"\"\n",
        "    Find the next 10 words with the maximum cosine similarity for a user-provided specific word.\n",
        "\n",
        "    Parameters:\n",
        "    - target_word: The word for which to find the next 10 cosine similarity words.\n",
        "    - embeddings: Dictionary of word embeddings.\n",
        "    - top_n: Number of top words to retrieve for the target word (default is 10).\n",
        "\n",
        "    Returns:\n",
        "    - List of the next 10 words with the maximum cosine similarity for the target word or [\"Word not in Corpus\"].\n",
        "    \"\"\"\n",
        "    if target_word not in embeddings:\n",
        "        return [\"Word not in Corpus\"]\n",
        "\n",
        "    # Get the embedding vector for the target word\n",
        "    target_vector = embeddings[target_word]\n",
        "\n",
        "    # Calculate cosine similarities between the target word and all other words\n",
        "    cosine_similarities = [(word, cosine_similarity(target_vector, embeddings[word])) for word in embeddings.keys()]\n",
        "\n",
        "    # Find the top-N most similar words\n",
        "    top_n_words = nlargest(top_n + 1, cosine_similarities, key=lambda x: x[1])\n",
        "\n",
        "    # Exclude the target word itself from the results\n",
        "    top_n_words = [word for word, _ in top_n_words if word != target_word]\n",
        "\n",
        "    return top_n_words[:10]\n",
        "\n",
        "# Usage: Find the next 10 most similar words for a specific word\n",
        "user_target_word = 'italy'\n",
        "next_10_cosine_for_user_word = find_next_10_cosine_words_for_word(user_target_word, embed_whole_skipgram_neg, top_n=10)\n",
        "\n",
        "# Print the results\n",
        "if next_10_cosine_for_user_word == [\"Word not in Corpus\"]:\n",
        "    print(\"Word not in Corpus\")\n",
        "else:\n",
        "    print(f\"Next 10 similar words for user-provided word '{user_target_word}': {next_10_cosine_for_user_word}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvs83WEo9ija",
        "outputId": "d61c5883-6967-4b31-c6b1-ccb82f0b2d17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Next 10 similar words for user-provided word 'italy': ['husband', 'kuna', 'ashgabat', 'brother', 'hid', 'pineapple', 'warmest', 'seeing', 'listens', 'cows']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy(predictions, true_words):\n",
        "    \"\"\"\n",
        "    Calculate accuracy based on predictions and true words.\n",
        "\n",
        "    Parameters:\n",
        "    - predictions: List of predicted words.\n",
        "    - true_words: List of true words.\n",
        "\n",
        "    Returns:\n",
        "    - Accuracy as a percentage.\n",
        "    \"\"\"\n",
        "    total_trials = len(predictions)\n",
        "    total_correct = sum(1 for pred_word in predictions if pred_word in true_words)\n",
        "\n",
        "    accuracy = (total_correct / total_trials) * 100\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "# Usage:\n",
        "semantic_accuracy_neg_samp = calculate_accuracy(find_max_cosine_words(y_pred_neg_samp_country, embed_whole_skipgram_neg), [true_word[3] for true_word in capital_common_countries])\n",
        "semantic_accuracy_pos_samp = calculate_accuracy(find_max_cosine_words(y_pred_positive_samp_country, embed_whole_skipgram), [true_word[3] for true_word in capital_common_countries])\n",
        "semantic_accuracy_glove = calculate_accuracy(find_max_cosine_words(y_pred_glove_country, embed_total_glove), [true_word[3] for true_word in capital_common_countries])\n",
        "\n",
        "print(\"Semantic Accuracy of Skipgram : {:.10f}%\".format(semantic_accuracy_pos_samp))\n",
        "print(\"Semantic Accuracy of Skipgram Neg: {:.10f}%\".format(semantic_accuracy_neg_samp))\n",
        "print(\"Semantic Accuracy of Glove: {:.10f}%\".format(semantic_accuracy_glove))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfGq8S_19qJW",
        "outputId": "088d7ac2-a9e1-4b9b-ebe0-0f3e8ae6b719"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Semantic Accuracy of Skipgram : 14.6245059289%\n",
            "Semantic Accuracy of Skipgram Neg: 14.4268774704%\n",
            "Semantic Accuracy of Glove: 14.2292490119%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_file_path = '/content/word-test.v1.txt'\n",
        "output_file_path = '/content/word-test-cleaned.txt'\n",
        "\n",
        "# Open the input file for reading\n",
        "with open(input_file_path, 'r', encoding='utf-8') as input_file:\n",
        "    # Read all lines from the input file\n",
        "    lines = input_file.readlines()\n",
        "\n",
        "# Open the output file for writing\n",
        "with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
        "    # Write all lines except the first line to the output file\n",
        "    output_file.writelines(lines[1:])\n",
        "\n",
        "print(f\"First line removed and content saved to: {output_file_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkoTQFBj9v8A",
        "outputId": "5b2fe24b-9547-48d1-e227-527155e4a615"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First line removed and content saved to: /content/word-test-cleaned.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "put_file_path = '/content/word-test.v1.txt'\n",
        "output_file_path = '/content/capital-common-countries.txt'\n",
        "\n",
        "# Open the input file for reading\n",
        "with open(input_file_path, 'r', encoding='utf-8') as input_file:\n",
        "    # Read all lines from the input file into a list\n",
        "    lines = input_file.readlines()\n",
        "\n",
        "# Open the output file for writing\n",
        "with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
        "    # Flag to indicate whether the relevant section should be written\n",
        "    start_writing = False\n",
        "\n",
        "    # Iterate through each line in the input file\n",
        "    for line in lines:\n",
        "        # If the line starts with ': capital-common-countries', set the flag to start writing\n",
        "        if line.startswith(': capital-common-countries'):\n",
        "            start_writing = True\n",
        "        # If a new section header is encountered (line starts with ':'), stop writing\n",
        "        elif line.startswith(':'):\n",
        "            start_writing = False\n",
        "\n",
        "        # Write the line to the output file if within the relevant section\n",
        "        if start_writing:\n",
        "            output_file.write(line)\n",
        "\n",
        "print(f\"Lines starting with ': capital-common-countries' saved to: {output_file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZfzGH5Q-BYS",
        "outputId": "6a5aaf8b-e6b7-45ad-d772-82b2dd7fb45d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lines starting with ': capital-common-countries' saved to: /content/capital-common-countries.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analogy_score_sem = model_gensim.evaluate_word_analogies(datapath('/content/capital-common-countries.txt'))\n",
        "print(\"Semtatical Accuracy of Model Gensim:\", analogy_score_sem[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyxMVjyX-ONR",
        "outputId": "2da28996-c496-412f-ecf3-9f744f16cac9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Semtatical Accuracy of Model Gensim: 0.9387351778656127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy(predictions, true_words):\n",
        "    \"\"\"\n",
        "    Calculate the accuracy of predictions compared to true words.\n",
        "\n",
        "    Parameters:\n",
        "    - predictions: List of predicted words generated by the model.\n",
        "    - true_words: List of ground-truth words to compare against.\n",
        "\n",
        "    Returns:\n",
        "    - Accuracy as a percentage value.\n",
        "    \"\"\"\n",
        "    total_trials = len(predictions)  # Total number of predictions made\n",
        "    total_correct = sum(1 for pred_word, true_word in zip(predictions, true_words) if pred_word == true_word)  # Count correct predictions\n",
        "\n",
        "    accuracy = (total_correct / total_trials) * 100  # Compute accuracy percentage\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "# Usage: Calculate syntactic accuracy for different models\n",
        "syntatical_accuracy_neg_samp = calculate_accuracy(find_max_cosine_words(y_pred_neg_samp_past, embed_whole_skipgram_neg), [true_word[3] for true_word in past_tense])\n",
        "syntatical_accuracy_pos_samp = calculate_accuracy(find_max_cosine_words(y_pred_positive_past_tense, embed_whole_skipgram), [true_word[3] for true_word in past_tense])\n",
        "syntatical_accuracy_glove = calculate_accuracy(find_max_cosine_words(y_pred_glove_past, embed_total_glove), [true_word[3] for true_word in past_tense])\n",
        "\n",
        "# Print the syntactic accuracy results for each model\n",
        "print(\"Syntactic Accuracy of Skipgram Pos Sampling: {:.2f}%\".format(syntatical_accuracy_pos_samp))\n",
        "print(\"Syntactic Accuracy of Skipgram Neg Sampling: {:.2f}%\".format(syntatical_accuracy_neg_samp))\n",
        "print(\"Syntactic Accuracy of Glove: {:.2f}%\".format(syntatical_accuracy_glove))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0vuH0gY-VAA",
        "outputId": "251a3085-9caf-4b33-a1da-e96b8ee0fc9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Syntactic Accuracy of Skipgram Pos Sampling: 0.06%\n",
            "Syntactic Accuracy of Skipgram Neg Sampling: 0.00%\n",
            "Syntactic Accuracy of Glove: 0.13%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_file_path = '/content/word-test.v1.txt'\n",
        "output_file_path = '/content/text_past_tense.txt'\n",
        "\n",
        "# Open the input file for reading\n",
        "with open(input_file_path, 'r', encoding='utf-8') as input_file:\n",
        "    # Read all lines from the input file\n",
        "    lines = input_file.readlines()\n",
        "\n",
        "# Open the output file for writing\n",
        "with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
        "    # Flag to indicate whether the relevant section (gram7-past-tense) should be written\n",
        "    start_writing = False\n",
        "\n",
        "    # Iterate through each line in the input file\n",
        "    for line in lines:\n",
        "        # If the line starts with ': gram7-past-tense', set the flag to start writing\n",
        "        if line.startswith(': gram7-past-tense'):\n",
        "            start_writing = True\n",
        "        # If a new section header is encountered (line starts with ':'), stop writing\n",
        "        elif line.startswith(':'):\n",
        "            start_writing = False\n",
        "\n",
        "        # Write the line to the output file if within the relevant section\n",
        "        if start_writing:\n",
        "            output_file.write(line)\n",
        "\n",
        "print(f\"The relevant lines have been saved to: {output_file_path}\")\n",
        "\n",
        "def calculate_accuracy(predictions, true_words):\n",
        "    \"\"\"\n",
        "    Calculate the accuracy of predictions compared to true words.\n",
        "\n",
        "    Parameters:\n",
        "    - predictions: List of predicted words generated by the model.\n",
        "    - true_words: List of ground-truth words to compare against.\n",
        "\n",
        "    Returns:\n",
        "    - Accuracy as a percentage value.\n",
        "    \"\"\"\n",
        "    total_trials = len(predictions)  # Total number of predictions made\n",
        "    total_correct = sum(1 for pred_word, true_word in zip(predictions, true_words) if pred_word == true_word)  # Count correct predictions\n",
        "\n",
        "    accuracy = (total_correct / total_trials) * 100  # Compute accuracy percentage\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "# Usage: Calculate syntactic accuracy for different models\n",
        "syntatical_accuracy_neg_samp = calculate_accuracy(find_max_cosine_words(y_pred_neg_samp_past, embed_whole_skipgram_neg), [true_word[3] for true_word in past_tense])\n",
        "syntatical_accuracy_pos_samp = calculate_accuracy(find_max_cosine_words(y_pred_positive_past_tense, embed_whole_skipgram), [true_word[3] for true_word in past_tense])\n",
        "syntatical_accuracy_glove = calculate_accuracy(find_max_cosine_words(y_pred_glove_past, embed_total_glove), [true_word[3] for true_word in past_tense])\n",
        "\n",
        "# Print the syntactic accuracy results for each model\n",
        "print(\"Syntactic Accuracy of Skipgram Negative Sampling: {:.2f}%\".format(syntatical_accuracy_neg_samp))\n",
        "print(\"Syntactic Accuracy of Skipgram Positive Sampling: {:.2f}%\".format(syntatical_accuracy_pos_samp))\n",
        "print(\"Syntactic Accuracy of Glove: {:.2f}%\".format(syntatical_accuracy_glove))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UqG_Y3Q-aCh",
        "outputId": "6ffe4615-ddb1-42b8-c3aa-b61a1d6567ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The relevant lines have been saved to: /content/text_past_tense.txt\n",
            "Syntactic Accuracy of Skipgram Negative Sampling: 0.00%\n",
            "Syntactic Accuracy of Skipgram Positive Sampling: 0.06%\n",
            "Syntactic Accuracy of Glove: 0.13%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analogy_score_syn = model_gensim.evaluate_word_analogies(datapath('/content/text_past_tense.txt'))\n",
        "print(\"Syntatical Accuracy of Model Gensim:\", analogy_score_syn[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOy04aVt-kFM",
        "outputId": "7876f9fd-2503-4cd0-f950-91aa57d671f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Syntatical Accuracy of Model Gensim: 0.5544871794871795\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "HR4-A1fHPoP0",
        "outputId": "c93e6e69-3eae-4b81-acc9-022637451027"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c715d4a9-9cce-42cd-9e8e-2bc5d29d9cea\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c715d4a9-9cce-42cd-9e8e-2bc5d29d9cea\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving wordsim_similarity_goldstandard.txt to wordsim_similarity_goldstandard (1).txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = 'wordsim_similarity_goldstandard.txt'\n",
        "\n",
        "# Define the column names\n",
        "columns = ['word_1', 'word_2', 'similarity_index']\n",
        "\n",
        "# Read the text file into a pandas DataFrame with specified column names\n",
        "df = pd.read_csv(file_path, sep='\\t', header=None, names=columns)\n",
        "\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "BbycP2P5-nj_",
        "outputId": "5b6838f8-4bfd-4002-bb28-21759892ab19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         word_1    word_2  similarity_index\n",
              "0         tiger       cat              7.35\n",
              "1         tiger     tiger             10.00\n",
              "2         plane       car              5.77\n",
              "3         train       car              6.31\n",
              "4    television     radio              6.77\n",
              "..          ...       ...               ...\n",
              "198     rooster    voyage              0.62\n",
              "199        noon    string              0.54\n",
              "200       chord     smile              0.54\n",
              "201   professor  cucumber              0.31\n",
              "202        king   cabbage              0.23\n",
              "\n",
              "[203 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c8b9e54a-2ecd-46e5-9bf6-01dfe9d091a6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word_1</th>\n",
              "      <th>word_2</th>\n",
              "      <th>similarity_index</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tiger</td>\n",
              "      <td>cat</td>\n",
              "      <td>7.35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>tiger</td>\n",
              "      <td>tiger</td>\n",
              "      <td>10.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>plane</td>\n",
              "      <td>car</td>\n",
              "      <td>5.77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>train</td>\n",
              "      <td>car</td>\n",
              "      <td>6.31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>television</td>\n",
              "      <td>radio</td>\n",
              "      <td>6.77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>rooster</td>\n",
              "      <td>voyage</td>\n",
              "      <td>0.62</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199</th>\n",
              "      <td>noon</td>\n",
              "      <td>string</td>\n",
              "      <td>0.54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200</th>\n",
              "      <td>chord</td>\n",
              "      <td>smile</td>\n",
              "      <td>0.54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>201</th>\n",
              "      <td>professor</td>\n",
              "      <td>cucumber</td>\n",
              "      <td>0.31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202</th>\n",
              "      <td>king</td>\n",
              "      <td>cabbage</td>\n",
              "      <td>0.23</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>203 rows × 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c8b9e54a-2ecd-46e5-9bf6-01dfe9d091a6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c8b9e54a-2ecd-46e5-9bf6-01dfe9d091a6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c8b9e54a-2ecd-46e5-9bf6-01dfe9d091a6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f8967411-310a-4f0b-bba4-6bb080070f41\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f8967411-310a-4f0b-bba4-6bb080070f41')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f8967411-310a-4f0b-bba4-6bb080070f41 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_66c02ca7-7e53-449e-905f-7259c2154987\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_66c02ca7-7e53-449e-905f-7259c2154987 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 203,\n  \"fields\": [\n    {\n      \"column\": \"word_1\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 130,\n        \"samples\": [\n          \"marathon\",\n          \"century\",\n          \"vodka\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"word_2\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 177,\n        \"samples\": [\n          \"Jackson\",\n          \"fauna\",\n          \"interview\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"similarity_index\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.5039610876799423,\n        \"min\": 0.23,\n        \"max\": 10.0,\n        \"num_unique_values\": 150,\n        \"samples\": [\n          8.34,\n          6.63,\n          3.04\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_embed(model_skipgram_neg,'<UNK>')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWfJhoeP-tmb",
        "outputId": "959aa53c-7683-41a1-ff22-d1844f75184d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.19411882758140564, -0.42883604764938354)"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate through each row in the DataFrame\n",
        "for index, row in df.iterrows():\n",
        "    word_1 = row['word_1']\n",
        "    word_2 = row['word_2']\n",
        "\n",
        "    try:\n",
        "        # Attempt to get embeddings and compute the dot product for various models\n",
        "        embed_1_neg_samp = get_embed(model_skipgram_neg, word_1)\n",
        "        embed_2_neg_samp = get_embed(model_skipgram_neg, word_2)\n",
        "        embed_1_pos_samp = get_embed(model_skipgram, word_1)\n",
        "        embed_2_pos_samp = get_embed(model_skipgram, word_2)\n",
        "        embed_1_glove = get_embed(model_glove, word_1)\n",
        "        embed_2_glove = get_embed(model_glove, word_2)\n",
        "\n",
        "    except KeyError:\n",
        "        # Handle missing words by substituting with '<UNK>' embeddings\n",
        "        embed_1_neg_samp = get_embed(model_skipgram_neg, '<UNK>')\n",
        "        embed_2_neg_samp = get_embed(model_skipgram_neg, '<UNK>')\n",
        "        embed_1_pos_samp = get_embed(model_skipgram, '<UNK>')\n",
        "        embed_2_pos_samp = get_embed(model_skipgram, '<UNK>')\n",
        "        embed_1_glove = get_embed(model_glove, '<UNK>')\n",
        "        embed_2_glove = get_embed(model_glove, '<UNK>')\n",
        "\n",
        "    # Compute and store the dot product results in the DataFrame\n",
        "    df.at[index, 'dot_product_neg_samp'] = np.dot(embed_1_neg_samp, embed_2_neg_samp)\n",
        "    df.at[index, 'dot_product_pos_samp'] = np.dot(embed_1_pos_samp, embed_2_pos_samp)\n",
        "    df.at[index, 'dot_product_glove'] = np.dot(embed_1_glove, embed_2_glove)\n",
        "\n",
        "# Display the first 10 rows of the updated DataFrame\n",
        "df[:10]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "TIme2mPDAa8h",
        "outputId": "18419169-d019-4bcf-80c8-6073a7c7d30b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       word_1  word_2  similarity_index  dot_product_neg_samp  \\\n",
              "0       tiger     cat              7.35              0.221582   \n",
              "1       tiger   tiger             10.00              0.221582   \n",
              "2       plane     car              5.77              0.221582   \n",
              "3       train     car              6.31              0.221582   \n",
              "4  television   radio              6.77              0.147957   \n",
              "5       media   radio              7.42              0.221582   \n",
              "6       bread  butter              6.19              0.221582   \n",
              "7    cucumber  potato              5.92              0.221582   \n",
              "8      doctor   nurse              7.00             -0.225749   \n",
              "9   professor  doctor              6.62             -0.750224   \n",
              "\n",
              "   dot_product_pos_samp  dot_product_glove  \n",
              "0              2.313294           0.719298  \n",
              "1              2.313294           0.719298  \n",
              "2              2.313294           0.719298  \n",
              "3              2.313294           0.719298  \n",
              "4             -0.040519           0.349478  \n",
              "5              2.313294           0.719298  \n",
              "6              2.313294           0.719298  \n",
              "7              2.313294           0.719298  \n",
              "8             -0.023560           0.435340  \n",
              "9              1.038951           0.280645  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c8bd05a8-ad40-4dcc-8e25-4de40b3c8d27\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word_1</th>\n",
              "      <th>word_2</th>\n",
              "      <th>similarity_index</th>\n",
              "      <th>dot_product_neg_samp</th>\n",
              "      <th>dot_product_pos_samp</th>\n",
              "      <th>dot_product_glove</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tiger</td>\n",
              "      <td>cat</td>\n",
              "      <td>7.35</td>\n",
              "      <td>0.221582</td>\n",
              "      <td>2.313294</td>\n",
              "      <td>0.719298</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>tiger</td>\n",
              "      <td>tiger</td>\n",
              "      <td>10.00</td>\n",
              "      <td>0.221582</td>\n",
              "      <td>2.313294</td>\n",
              "      <td>0.719298</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>plane</td>\n",
              "      <td>car</td>\n",
              "      <td>5.77</td>\n",
              "      <td>0.221582</td>\n",
              "      <td>2.313294</td>\n",
              "      <td>0.719298</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>train</td>\n",
              "      <td>car</td>\n",
              "      <td>6.31</td>\n",
              "      <td>0.221582</td>\n",
              "      <td>2.313294</td>\n",
              "      <td>0.719298</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>television</td>\n",
              "      <td>radio</td>\n",
              "      <td>6.77</td>\n",
              "      <td>0.147957</td>\n",
              "      <td>-0.040519</td>\n",
              "      <td>0.349478</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>media</td>\n",
              "      <td>radio</td>\n",
              "      <td>7.42</td>\n",
              "      <td>0.221582</td>\n",
              "      <td>2.313294</td>\n",
              "      <td>0.719298</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>bread</td>\n",
              "      <td>butter</td>\n",
              "      <td>6.19</td>\n",
              "      <td>0.221582</td>\n",
              "      <td>2.313294</td>\n",
              "      <td>0.719298</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>cucumber</td>\n",
              "      <td>potato</td>\n",
              "      <td>5.92</td>\n",
              "      <td>0.221582</td>\n",
              "      <td>2.313294</td>\n",
              "      <td>0.719298</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>doctor</td>\n",
              "      <td>nurse</td>\n",
              "      <td>7.00</td>\n",
              "      <td>-0.225749</td>\n",
              "      <td>-0.023560</td>\n",
              "      <td>0.435340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>professor</td>\n",
              "      <td>doctor</td>\n",
              "      <td>6.62</td>\n",
              "      <td>-0.750224</td>\n",
              "      <td>1.038951</td>\n",
              "      <td>0.280645</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c8bd05a8-ad40-4dcc-8e25-4de40b3c8d27')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c8bd05a8-ad40-4dcc-8e25-4de40b3c8d27 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c8bd05a8-ad40-4dcc-8e25-4de40b3c8d27');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-887f58c6-f414-4689-9a35-f59d2719fc24\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-887f58c6-f414-4689-9a35-f59d2719fc24')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-887f58c6-f414-4689-9a35-f59d2719fc24 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df[:10]\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"word_1\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"doctor\",\n          \"plane\",\n          \"bread\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"word_2\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"tiger\",\n          \"potato\",\n          \"cat\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"similarity_index\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.2140588490221094,\n        \"min\": 5.77,\n        \"max\": 10.0,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          7.0,\n          10.0,\n          7.42\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dot_product_neg_samp\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.32095331825441864,\n        \"min\": -0.7502236787637742,\n        \"max\": 0.22158247498512385,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.14795688527421902,\n          -0.7502236787637742,\n          0.22158247498512385\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dot_product_pos_samp\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0037229051256114,\n        \"min\": -0.04051946893886216,\n        \"max\": 2.313293726383308,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          -0.04051946893886216,\n          1.0389512431805272,\n          2.313293726383308\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dot_product_glove\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1796521911485303,\n        \"min\": 0.28064540685228323,\n        \"max\": 0.7192977669542566,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.3494783306915341,\n          0.28064540685228323,\n          0.7192977669542566\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import spearmanr\n",
        "\n",
        "# Calculate the Spearman correlation between similarity scores and dot products from different models\n",
        "correlation_neg, _ = spearmanr(df['similarity_index'], df['dot_product_neg_samp'])\n",
        "correlation_pos, _ = spearmanr(df['similarity_index'], df['dot_product_pos_samp'])\n",
        "correlation_glove, _ = spearmanr(df['similarity_index'], df['dot_product_glove'])\n",
        "\n",
        "# Print the Spearman correlation coefficients for each model\n",
        "print(f\"Spearman Correlation Coefficient of Skipgram Negative Sampling: {correlation_neg:.4f}\")\n",
        "print(f\"Spearman Correlation Coefficient of Skipgram Positive Sampling: {correlation_pos:.4f}\")\n",
        "print(f\"Spearman Correlation Coefficient of Glove: {correlation_glove:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7KEvI3-Ad1_",
        "outputId": "41d4397f-383b-433f-e22b-7051a4705edb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spearman Correlation Coefficient of Skipgram Negative Sampling: 0.0657\n",
            "Spearman Correlation Coefficient of Skipgram Positive Sampling: 0.0222\n",
            "Spearman Correlation Coefficient of Glove: -0.0012\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding y_true based on the mean of similarity index in the df\n",
        "y_true = df['similarity_index'].mean()\n",
        "\n",
        "print(f\"y_true: {y_true:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giT0uAhpAggk",
        "outputId": "7913cbe3-7ac9-4a75-e219-e1d6203e7eb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_true: 5.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the correlation coeffiecient of the gensim model using the predefined function\n",
        "correlation_coefficient = model_gensim.evaluate_word_pairs(datapath('/content/wordsim_similarity_goldstandard.txt'))\n",
        "print(f\"Correlation coefficient: {correlation_coefficient[1][0]:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oST9Ui4tAif4",
        "outputId": "206d1896-330e-4a54-fcc6-adce253f067e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correlation coefficient: 0.60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embed_whole_glove = get_embed_for_corpus(model_glove, vocab)\n",
        "embed_whole_neg_skg = get_embed_for_corpus(model_skipgram_neg, vocab)\n",
        "embed_whole_pos_skg = get_embed_for_corpus(model_skipgram, vocab)"
      ],
      "metadata": {
        "id": "cpo3VBfMAnM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Save the Gensim model to a file using pickle\n",
        "gensim_model_path = 'model_gensim.pkl'\n",
        "\n",
        "with open(gensim_model_path, 'wb') as model_file:\n",
        "    pickle.dump(model_gensim, model_file)\n",
        "\n",
        "print(f\"Gensim model saved to: {gensim_model_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvUrfCi7AwMA",
        "outputId": "6006419d-8256-49d5-decc-4ccb2194a19e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gensim model saved to: model_gensim.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the path to your pickled Gensim model file\n",
        "gensim_model_path = 'model_gensim.pkl'\n",
        "\n",
        "# Load the Gensim model from the pickle file\n",
        "with open(gensim_model_path, 'rb') as model_file:\n",
        "    loaded_model = pickle.load(model_file)\n",
        "for i in range (1,10):\n",
        "    print(loaded_model.most_similar('language')[i][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SY9IvU97Azm1",
        "outputId": "352e5632-c405-441d-a051-cbc309096eed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word\n",
            "spoken\n",
            "arabic\n",
            "english\n",
            "dialect\n",
            "vocabulary\n",
            "text\n",
            "translation\n",
            "words\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Save the embeddings for Skipgram Positive Sampling\n",
        "embedding_dict = embed_whole_pos_skg  # Embedding dictionary for the positive sampling model\n",
        "\n",
        "# Specify the file path for the pickle file\n",
        "pickle_file_path = 'embed_skipgram_pos.pkl'\n",
        "\n",
        "# Save the embedding dictionary to a pickle file\n",
        "with open(pickle_file_path, 'wb') as pickle_file:\n",
        "    pickle.dump(embedding_dict, pickle_file)\n",
        "\n",
        "print(f\"Embedding dictionary has been saved to: {pickle_file_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1L-7KOz6A5tx",
        "outputId": "9c0a3182-a2ba-4dfe-d50b-690fb5ea3585"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding dictionary has been saved to: embed_skipgram_pos.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Save the embeddings for Skipgram Negative Sampling\n",
        "embedding_dict = embed_whole_neg_skg  # Embedding dictionary for the negative sampling model\n",
        "\n",
        "# Specify the file path for the pickle file\n",
        "pickle_file_path = 'embed_skipgram_neg.pkl'\n",
        "\n",
        "# Open the file in binary write mode and save the embedding dictionary\n",
        "with open(pickle_file_path, 'wb') as pickle_file:\n",
        "    pickle.dump(embedding_dict, pickle_file)\n",
        "\n",
        "# Confirm that the embedding dictionary has been saved successfully\n",
        "print(f\"Embedding dictionary for negative sampling has been saved to: {pickle_file_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANIvnaLNBFSH",
        "outputId": "3af8f3e2-873e-4dfb-f24f-2763ff272bd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding dictionary for negative sampling has been saved to: embed_skipgram_neg.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Save the embeddings for the GloVe model\n",
        "embedding_dict = embed_whole_glove  # Embedding dictionary for the GloVe model\n",
        "\n",
        "# Specify the file path for the pickle file\n",
        "pickle_file_path = 'embed_glove.pkl'\n",
        "\n",
        "# Open the file in binary write mode and save the embedding dictionary\n",
        "with open(pickle_file_path, 'wb') as pickle_file:\n",
        "    pickle.dump(embedding_dict, pickle_file)\n",
        "\n",
        "# Confirm that the embedding dictionary has been saved successfully\n",
        "print(f\"Embedding dictionary for GloVe has been saved to: {pickle_file_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1dfvOiaBIsQ",
        "outputId": "43da89d2-3efe-461f-c711-fdc49e0c6309"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding dictionary for GloVe has been saved to: embed_glove.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Specify the path to the pickled file for Skipgram Positive Sampling embeddings\n",
        "pickle_file_path = 'embed_skipgram_pos.pkl'\n",
        "\n",
        "# Load the embedding dictionary for Skipgram Positive Sampling from the pickle file\n",
        "with open(pickle_file_path, 'rb') as pickle_file:\n",
        "    embedding_dict_neg = pickle.load(pickle_file)\n",
        "\n",
        "import pickle\n",
        "\n",
        "# Specify the path to the pickled file for Skipgram Negative Sampling embeddings\n",
        "pickle_file_path = 'embed_skipgram_neg.pkl'\n",
        "\n",
        "# Load the embedding dictionary for Skipgram Negative Sampling from the pickle file\n",
        "with open(pickle_file_path, 'rb') as pickle_file:\n",
        "    embedding_dict_pos = pickle.load(pickle_file)\n",
        "\n",
        "import pickle\n",
        "\n",
        "# Specify the path to the pickled file for GloVe embeddings\n",
        "pickle_file_path = 'embed_glove.pkl'\n",
        "\n",
        "# Load the embedding dictionary for GloVe from the pickle file\n",
        "with open(pickle_file_path, 'rb') as pickle_file:\n",
        "    embedding_dict_glove = pickle.load(pickle_file)\n",
        "\n",
        "# Define a target word for similarity calculation\n",
        "user_target_word = \"run\"\n",
        "\n",
        "# Find the next 10 most similar words to the target word using the GloVe embeddings\n",
        "next_10_cosine_for_user_word = find_next_10_cosine_words_for_word(user_target_word, embedding_dict_glove, top_n=10)\n",
        "\n",
        "# Display the results\n",
        "if next_10_cosine_for_user_word == [\"Word not in Corpus\"]:\n",
        "    print(\"The target word is not found in the corpus.\")\n",
        "else:\n",
        "    print(f\"Next 10 similar words for the target word '{user_target_word}': {next_10_cosine_for_user_word}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4JpMokFBNN9",
        "outputId": "6de5d7f7-dcfa-478a-e5ca-26bc7718caec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Next 10 similar words for the target word 'run': ['molvar', \"wouldn't\", 'inflate', 'letters', 'understand', 'know', 'couple', 'published', 'salinger', 'raised']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Comparison and Analysis\n",
        "\n",
        "### **Training Loss and Runtime**\n",
        "\n",
        "| Model             | Window Size | Training Loss (Last Epoch) | Training Time (s) |\n",
        "|--------------------|-------------|----------------------------|--------------------|\n",
        "| Skipgram (Positive)| 2           | 11.756741                  | 1.99              |\n",
        "| Skipgram (Negative)| 2           | 8.751039                   | 1.89              |\n",
        "| GloVe             | 2           | 2.034088                   | 1.89              |\n",
        "| GloVe (Gensim)    | -           | -                          | -                 |\n",
        "\n",
        "---\n",
        "\n",
        "### **Semantic Accuracy**\n",
        "\n",
        "| Model                | Accuracy (%)         |\n",
        "|-----------------------|----------------------|\n",
        "| Skipgram             | 14.62               |\n",
        "| Skipgram (Negative)  | 14.43               |\n",
        "| GloVe               | 14.23               |\n",
        "| GloVe (Gensim)      | 0.93                |\n",
        "\n",
        "---\n",
        "\n",
        "### **Syntactic Accuracy**\n",
        "\n",
        "| Model                | Accuracy (%)         |\n",
        "|-----------------------|----------------------|\n",
        "| Skipgram (Positive)  | 0.06                |\n",
        "| Skipgram (Negative)  | 0.00                |\n",
        "| GloVe               | 0.13                |\n",
        "| GloVe (Gensim)      | 0.55                |\n",
        "\n",
        "---\n",
        "\n",
        "### **Notes:**\n",
        "- **Semantic accuracy** is calculated using the **capital-common-countries** dataset.\n",
        "- **Syntactic accuracy** is evaluated using the **gram7-past-tense** dataset.\n",
        "- The results may vary due to corpus limitations. Using a pre-trained model such as **GloVe (Gensim)** might yield better results.\n",
        "- Training times are approximate and may depend on hardware.\n",
        "\n"
      ],
      "metadata": {
        "id": "aY4AyUOlcVnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install streamlit\n"
      ],
      "metadata": {
        "id": "s9jT570BBXPK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d310145-68b0-438a-b001-ad8f046dbb31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.41.1-py2.py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.1.8)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.1.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.25.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (17.0.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (13.9.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.12.2)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.5)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.21.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2024.12.14)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.22.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.41.1-py2.py3-none-any.whl (9.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.41.1 watchdog-6.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import streamlit as st\n",
        "import numpy as np\n",
        "\n",
        "# Define a function to compute cosine similarity\n",
        "def compute_cosine_similarity(vector1, vector2):\n",
        "    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n",
        "    dot_prod = np.dot(vector1, vector2)\n",
        "    norm1 = np.linalg.norm(vector1)\n",
        "    norm2 = np.linalg.norm(vector2)\n",
        "    return dot_prod / (norm1 * norm2) if norm1 > 0 and norm2 > 0 else 0.0\n",
        "\n",
        "# Load pre-saved embedding dictionaries\n",
        "def load_embedding_files():\n",
        "    \"\"\"Load embeddings for different models from pickle files.\"\"\"\n",
        "    positive_path = 'embed_skipgram_pos.pkl'\n",
        "    negative_path = 'embed_skipgram_neg.pkl'\n",
        "    glove_path = 'embed_glove.pkl'\n",
        "\n",
        "    with open(positive_path, 'rb') as pos_file:\n",
        "        pos_embeddings = pickle.load(pos_file)\n",
        "    with open(negative_path, 'rb') as neg_file:\n",
        "        neg_embeddings = pickle.load(neg_file)\n",
        "    with open(glove_path, 'rb') as glove_file:\n",
        "        glove_embeddings = pickle.load(glove_file)\n",
        "\n",
        "    return pos_embeddings, neg_embeddings, glove_embeddings\n",
        "\n",
        "# Find the most similar words for a given word\n",
        "def find_similar_words(target_word, embedding_dict, top_n=10):\n",
        "    \"\"\"\n",
        "    Identify top N words with the highest cosine similarity to the target word.\n",
        "\n",
        "    Parameters:\n",
        "    - target_word: Word for which similar words are sought.\n",
        "    - embedding_dict: Dictionary containing word embeddings.\n",
        "    - top_n: Number of similar words to retrieve.\n",
        "\n",
        "    Returns:\n",
        "    - List of the most similar words.\n",
        "    \"\"\"\n",
        "    if target_word not in embedding_dict:\n",
        "        return [\"Word not in Corpus\"]\n",
        "\n",
        "    target_vec = embedding_dict[target_word]\n",
        "    similarities = []\n",
        "\n",
        "    for word, vec in embedding_dict.items():\n",
        "        if word != target_word:  # Exclude the target word itself\n",
        "            similarity = compute_cosine_similarity(target_vec, vec)\n",
        "            similarities.append((word, similarity))\n",
        "\n",
        "    # Rank words by similarity score in descending order\n",
        "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return [word for word, _ in similarities[:top_n]]\n",
        "\n",
        "# Main Streamlit app functionality\n",
        "def main():\n",
        "    # Load embedding dictionaries for all models\n",
        "    pos_embeddings, neg_embeddings, glove_embeddings = load_embedding_files()\n",
        "\n",
        "    # Display the app title and description\n",
        "    st.title(\"Word Similarity Finder\")\n",
        "    st.write(\"Enter a word and choose a model to find similar words based on cosine similarity.\")\n",
        "\n",
        "    # Input field for user to type a word\n",
        "    input_word = st.text_input(\"Type a word:\", \"example\")  # Default word is \"example\"\n",
        "\n",
        "    # Dropdown menu to select the embedding model\n",
        "    selected_model = st.selectbox(\n",
        "        \"Select Embedding Model\",\n",
        "        [\"GloVe Embeddings\", \"Skipgram Positive Embeddings\", \"Skipgram Negative Embeddings\"]\n",
        "    )\n",
        "\n",
        "    # Select the appropriate embedding dictionary\n",
        "    if selected_model == \"GloVe Embeddings\":\n",
        "        embeddings = glove_embeddings\n",
        "    elif selected_model == \"Skipgram Positive Embeddings\":\n",
        "        embeddings = pos_embeddings\n",
        "    elif selected_model == \"Skipgram Negative Embeddings\":\n",
        "        embeddings = neg_embeddings\n",
        "\n",
        "    # Display the top 10 similar words if input is provided\n",
        "    if input_word:\n",
        "        with st.spinner('Processing your request...'):\n",
        "            similar_words = find_similar_words(input_word, embeddings, top_n=10)\n",
        "\n",
        "            # Show results\n",
        "            if similar_words == [\"Word not in Corpus\"]:\n",
        "                st.error(\"The word you entered is not in the corpus.\")\n",
        "            else:\n",
        "                st.success(f\"Top 10 similar words for '{input_word}':\")\n",
        "                st.write(similar_words)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "O5AkAvRC672i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ef0689c-14f5-47cc-aa64-97ddbc0e7cde"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-01-18 02:32:18.664 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-18 02:32:18.789 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py [ARGUMENTS]\n",
            "2025-01-18 02:32:18.799 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-18 02:32:18.801 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-18 02:32:18.802 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-18 02:32:18.805 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-18 02:32:18.807 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-18 02:32:18.808 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-18 02:32:18.809 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-18 02:32:18.811 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-18 02:32:18.812 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-18 02:32:18.813 Session state does not function when running a script without `streamlit run`\n",
            "2025-01-18 02:32:18.814 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-18 02:32:18.815 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-18 02:32:18.817 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-18 02:32:18.818 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-18 02:32:18.819 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-18 02:32:18.820 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-18 02:32:18.823 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-18 02:32:18.825 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-18 02:32:18.826 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-18 02:32:18.828 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-18 02:32:18.829 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-18 02:32:18.914 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-18 02:32:18.921 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-18 02:32:18.928 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-18 02:32:18.930 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-18 02:32:18.933 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-18 02:32:18.935 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Save the Streamlit app as app.py in Colab's /content directory\n",
        "modified_code = \"\"\"\n",
        "import pickle\n",
        "import streamlit as st\n",
        "import numpy as np\n",
        "\n",
        "# Define a function to compute cosine similarity\n",
        "def compute_cosine_similarity(vector1, vector2):\n",
        "    \\\"\\\"\\\"Calculate cosine similarity between two vectors.\\\"\\\"\\\"\n",
        "    dot_prod = np.dot(vector1, vector2)\n",
        "    norm1 = np.linalg.norm(vector1)\n",
        "    norm2 = np.linalg.norm(vector2)\n",
        "    return dot_prod / (norm1 * norm2) if norm1 > 0 and norm2 > 0 else 0.0\n",
        "\n",
        "# Load pre-saved embedding dictionaries\n",
        "def load_embedding_files():\n",
        "    \\\"\\\"\\\"Load embeddings for different models from pickle files.\\\"\\\"\\\"\n",
        "    positive_path = 'embed_skipgram_pos.pkl'\n",
        "    negative_path = 'embed_skipgram_neg.pkl'\n",
        "    glove_path = 'embed_glove.pkl'\n",
        "\n",
        "    with open(positive_path, 'rb') as pos_file:\n",
        "        pos_embeddings = pickle.load(pos_file)\n",
        "    with open(negative_path, 'rb') as neg_file:\n",
        "        neg_embeddings = pickle.load(neg_file)\n",
        "    with open(glove_path, 'rb') as glove_file:\n",
        "        glove_embeddings = pickle.load(glove_file)\n",
        "\n",
        "    return pos_embeddings, neg_embeddings, glove_embeddings\n",
        "\n",
        "# Find the most similar words for a given word\n",
        "def find_similar_words(target_word, embedding_dict, top_n=10):\n",
        "    \\\"\\\"\\\"\n",
        "    Identify top N words with the highest cosine similarity to the target word.\n",
        "\n",
        "    Parameters:\n",
        "    - target_word: Word for which similar words are sought.\n",
        "    - embedding_dict: Dictionary containing word embeddings.\n",
        "    - top_n: Number of similar words to retrieve.\n",
        "\n",
        "    Returns:\n",
        "    - List of the most similar words.\n",
        "    \\\"\\\"\\\"\n",
        "    if target_word not in embedding_dict:\n",
        "        return [\\\"Word not in Corpus\\\"]\n",
        "\n",
        "    target_vec = embedding_dict[target_word]\n",
        "    similarities = []\n",
        "\n",
        "    for word, vec in embedding_dict.items():\n",
        "        if word != target_word:  # Exclude the target word itself\n",
        "            similarity = compute_cosine_similarity(target_vec, vec)\n",
        "            similarities.append((word, similarity))\n",
        "\n",
        "    # Rank words by similarity score in descending order\n",
        "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return [word for word, _ in similarities[:top_n]]\n",
        "\n",
        "# Main Streamlit app functionality\n",
        "def main():\n",
        "    # Load embedding dictionaries for all models\n",
        "    pos_embeddings, neg_embeddings, glove_embeddings = load_embedding_files()\n",
        "\n",
        "    # Display the app title and description\n",
        "    st.title(\"Word Similarity Finder\")\n",
        "    st.write(\"Enter a word and choose a model to find similar words based on cosine similarity.\")\n",
        "\n",
        "    # Input field for user to type a word\n",
        "    input_word = st.text_input(\"Type a word:\", \"example\")  # Default word is \"example\"\n",
        "\n",
        "    # Dropdown menu to select the embedding model\n",
        "    selected_model = st.selectbox(\n",
        "        \"Select Embedding Model\",\n",
        "        [\"GloVe Embeddings\", \"Skipgram Positive Embeddings\", \"Skipgram Negative Embeddings\"]\n",
        "    )\n",
        "\n",
        "    # Select the appropriate embedding dictionary\n",
        "    if selected_model == \"GloVe Embeddings\":\n",
        "        embeddings = glove_embeddings\n",
        "    elif selected_model == \"Skipgram Positive Embeddings\":\n",
        "        embeddings = pos_embeddings\n",
        "    elif selected_model == \"Skipgram Negative Embeddings\":\n",
        "        embeddings = neg_embeddings\n",
        "\n",
        "    # Display the top 10 similar words if input is provided\n",
        "    if input_word:\n",
        "        with st.spinner('Processing your request...'):\n",
        "            similar_words = find_similar_words(input_word, embeddings, top_n=10)\n",
        "\n",
        "            # Show results\n",
        "            if similar_words == [\\\"Word not in Corpus\\\"]:\n",
        "                st.error(\\\"The word you entered is not in the corpus.\\\")\n",
        "            else:\n",
        "                st.success(f\\\"Top 10 similar words for '{input_word}':\\\")\n",
        "                st.write(similar_words)\n",
        "\n",
        "if __name__ == \\\"__main__\\\":\n",
        "    main()\n",
        "\"\"\"\n",
        "\n",
        "# Write the modified code to app.py\n",
        "with open('/content/app.py', 'w') as f:\n",
        "    f.write(modified_code)\n",
        "\n",
        "print(\"Streamlit app has been saved as app.py in /content directory.\")\n"
      ],
      "metadata": {
        "id": "Lzx1eYz5671T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fea8397-bb43-4029-ebd7-fa714e515e34"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit app has been saved as app.py in /content directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Streamlit and pyngrok first\n",
        "!pip install pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0RQMfpeR8wF",
        "outputId": "13dbbf69-4ac4-4fba-e993-de17206e4dbc"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.3-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.3-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /root/.config/ngrok/ngrok.yml"
      ],
      "metadata": {
        "id": "FSOnYhuvZvBO"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken 2lVEdM8aMwFnpNeDzeqw2US1ZDZ_5VHYQQDKamomRpqWsm6R1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLM9ukNgZ1o6",
        "outputId": "f090812d-3dd1-49ae-eea3-b56d219b5111"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yvU9_BvXTipP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Start the ngrok tunnel explicitly specifying HTTP protocol\n",
        "public_url = ngrok.connect(addr=8501, proto=\"http\")\n",
        "print(f\"Public URL: {public_url}\")\n",
        "\n",
        "# Run the Streamlit app\n",
        "!streamlit run /content/app.py"
      ],
      "metadata": {
        "id": "drBiHEBh67vz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f6ee1a3-bc54-471a-caf6-26bb38ed1ebb"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: NgrokTunnel: \"https://2263-34-57-80-108.ngrok-free.app\" -> \"http://localhost:8501\"\n",
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.57.80.108:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j-OxZ9co67V_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}